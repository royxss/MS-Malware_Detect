#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on Sun Dec 23 10:36:31 2018

@author: roy
"""

import pandas as pd
pd.set_option('display.expand_frame_repr', False)
from dfply import *
import dask.dataframe as dd
from tqdm import tqdm

# Load data
#train = pd.read_csv("/home/roy/Downloads/Kaggle_Data/mmp/splitter/segmentaa")
train = dd.read_csv("/home/roy/Downloads/Kaggle_Data/mmp/train.csv",
                    dtype={'PuaMode': object},
                    assume_missing=True) #Using dask

# Check structure of data
train.info()
# Drop machine id
all_cols = list(train.columns)
all_cols.remove('MachineIdentifier')
train = train[all_cols]
train.columns

# Find all datatypes
df_col_series = train.dtypes
df_col = pd.DataFrame({'col':df_col_series.index, 'types':df_col_series.values})
df_col.types.unique()
# There are three different datatypes. float and int can be taken as numeric

# Setup numeric and categorical columns
df_col_category = df_col >> mask(X.types == 'O') >> pull('col')
len(df_col_category)
df_col_numeric = df_col >> mask(X.types != 'O') >> pull('col')
len(df_col_numeric)

# Remove target from columns list
target = 'HasDetections'  
df_col_numeric_wo_target = list(df_col_numeric)
df_col_numeric_wo_target.remove(target)

# Check target distribution
'''
train_mod = train
train_mod[target].apply(str)
temp = train_mod.groupby([target])[target].count().compute()
df = pd.DataFrame({target:temp.index, 'count_of':temp.values})
df >>= mutate(percent_of=X.count_of*100/len(train_mod))
print(df)
# Quiet evenly balanced
'''

'''
# Check unique elements in categorical columns
summary_cat = pd.DataFrame(columns = ['field_name', 'value', 'total_count', 'percent_of',
       'percent_of_hasDetection'])
def summarize_cat_columns(train, i):
    #i = 'OsVer'
    temp = train.groupby([i, target])[i].count().compute()
    df = pd.DataFrame(temp)
    df.columns = ['Count_of']
    df.reset_index(inplace=True)
    df = df >> spread(X.HasDetections, X.Count_of)
    df.columns = [i, 'noDetection', 'hasDetection']
    df = df.fillna(0)
    df >>= mutate(total_count = X.noDetection + X.hasDetection)
    df >>= mutate(percent_of=X.total_count*100/len(train))
    df >>= mutate(percent_of_noDetection=X.noDetection*100/X.total_count, percent_of_hasDetection=X.hasDetection*100/X.total_count)
    df = df.rename(columns={ df.columns[0]: "value" })
    df >>= mutate(field_name = i)
    df >>= select(X.field_name, X.value, X.total_count, X.percent_of, X.percent_of_hasDetection)
    print(df)
    return df
    
for i in tqdm(list(df_col_category)):
    summary_cat = pd.concat([summary_cat, summarize_cat_columns(train, i)])
'''    
#summary_cat.to_pickle("/home/roy/Downloads/Kaggle_Data/mmp/summary_cat.pkl")   
summary_cat = pd.read_pickle("/home/roy/Downloads/Kaggle_Data/mmp/summary_cat.pkl")  
# Print categories for details analysis
summary_cat_cols = list(summary_cat['field_name'].unique())

#pd.set_option('display.height', 1000)
#pd.set_option('display.max_rows', 500)
#pd.set_option('display.max_columns', 500)
#pd.set_option('display.width', 1000)

# Print all the cat columns and check data
for i in summary_cat_cols:
    print(summary_cat[summary_cat['field_name'] == i].sort_values(by='percent_of'))

''' 
AvSigVersion has 8531 categorical values. 
EngineVersion has 70
AppVersion has 110
OsBuildLab has 663
Census_InternalBatteryType has 78
Census_OSVersion has 469
Many categorical values with 0 detections can be removed. This would reduce features during one-hot encoding
'''

featureHashList = ['AvSigVersion', 'AppVersion', 'OsBuildLab', 'Census_OSVersion']

# Exclude AvSigVersion
excludeList = []
excludeListCat = []
excludeListCat.append('AvSigVersion')
excludeListCat.append('OsBuildLab')
excludeListCat.append('Census_OSVersion')
excludeListCat.append('AppVersion')

# Remove rows with 0 detections
drop_rows = summary_cat[summary_cat['percent_of_hasDetection'] == 0]
drop_rows = drop_rows.reset_index(drop=True)
#for ind in range(len(drop_rows)):
#    col = drop_rows['field_name'][ind]
#    val = drop_rows['value'][ind]
#    print(col+ ' : ' +str(val))
#    train[train['Census_FlightRing'] == 'OSG'].compute()
    
    
# Check categorical columns for Na, NA, NaN, Unknown, UNKNOWN, '' , Invalid
df = summary_cat[summary_cat['value'].isin(['Na', 'NA', 'NaN', 'Unknown', 
                 'UNKNOWN', '', 'na', 'Invalid', 'invalid', 'INVALID',
                 'Unspecified', 'UNSPECIFIED', 'Not Found'])] 
if len(df) > 0:
    print(df)
# Nothing to do here  

# Many columns have NaN values
'''
numeric_null = pd.DataFrame(columns=['column_name', 'percent_of'])
def summarize_num_columns_for_missing(train, i):
    #i = 'DefaultBrowsersIdentifier'
    temp = 100 * (train[i].isnull().sum().compute())/len(train)
    if temp > 0:
        return pd.DataFrame([[i, temp]], columns=['column_name', 'percent_of'])
    
for i in tqdm(list(df_col_numeric_wo_target)):
    numeric_null = pd.concat([numeric_null, summarize_num_columns_for_missing(train, i)])
    '''

#numeric_null.to_pickle("/home/roy/Downloads/Kaggle_Data/mmp/numeric_null.pkl") 
numeric_null = pd.read_pickle("/home/roy/Downloads/Kaggle_Data/mmp/numeric_null.pkl") 
numeric_null = numeric_null.sort_values(by = 'percent_of', ascending = False)  
# DefaultBrowsersIdentifier, Census_IsFlightingInternal, Census_ThresholdOptIn, Census_IsWIMBootEnabled
excludeList.append('DefaultBrowsersIdentifier') 
excludeList.append('Census_IsFlightingInternal')  
excludeList.append('Census_ThresholdOptIn')  
excludeList.append('Census_IsWIMBootEnabled')     
    
# Check corr matrix
'''
corr_mat = train[df_col_numeric_wo_target].corr().abs().compute()
corr_mat = corr_mat.unstack()
corr_sorted = corr_mat.sort_values(kind="quicksort")
corr_sorted = pd.DataFrame(corr_sorted)
corr_sorted.reset_index(inplace=True)
corr_sorted.columns = ['col1', 'col2','val']
corr_sorted >>= mask(X.val.isnull() == False)
corr_sorted >>= mutate(mark = X['col1'] == X['col2'])
corr_sorted['val'] = corr_sorted['val'].round(3)
corr_sorted >>= mask(X.mark == False) >> select(X.col1, X.col2, X.val)
'''

#corr_sorted.to_pickle("/home/roy/Downloads/Kaggle_Data/mmp/corr_sorted.pkl")
corr_sorted = pd.read_pickle("/home/roy/Downloads/Kaggle_Data/mmp/corr_sorted.pkl") 
'''
Census_OSInstallLanguageIdentifier and Census_OSUILocaleIdentifier has 98
Census_OSBuildNumber and OsBuild has 93
Census_InternalPrimaryDisplayResolutionVertical and Census_InternalPrimaryDisplayResolutionHorizontal has 90
'''
# Print again the above columns to see which makes more sense to drop
corr_cols = ['Census_OSInstallLanguageIdentifier', 'Census_OSUILocaleIdentifier', 'Census_OSBuildNumber', 'OsBuild', 'Census_InternalPrimaryDisplayResolutionVertical', 'Census_InternalPrimaryDisplayResolutionHorizontal']
'''train[corr_cols].describe().compute()'''
# Among the correlated columns, will drop the columns with lowest variance.
excludeList.append('Census_OSInstallLanguageIdentifier') 
excludeList.append('Census_OSBuildNumber') 
excludeList.append('Census_InternalPrimaryDisplayResolutionVertical') 

# Check distribution of numeric
'''desc_num_cols = train.describe().compute()
desc_num_cols.to_csv("/home/roy/Downloads/Kaggle_Data/mmp/desc_num_cols.csv")  
'''
'''
IsBeta, AutoSampleOptIn , Census_IsFlightingInternal, Census_IsFlightsDisabled, Census_IsWIMBootEnabled (very low) has near zero variance.
Scaling may be required as distribution is skewed
'''
excludeList.append('IsBeta') 
excludeList.append('AutoSampleOptIn') 
excludeList.append('Census_IsFlightingInternal') 
excludeList.append('Census_IsFlightsDisabled') 
excludeList.append('Census_IsWIMBootEnabled') 
  

from dask.distributed import Client
client = Client()
#import dask_ml.joblib
from sklearn.externals.joblib import parallel_backend
from dask_ml.xgboost import XGBClassifier

final_cat = list(set(list(df_col_category)).difference(set(excludeListCat)))
final_num = list(set(list(df_col_numeric_wo_target)).difference(set(excludeList)))

final_cols = final_cat + final_num
final_cols.append(target)
data = train[final_cols]

all_cat_cols = final_cat
all_cat_cols.append(target)

dd_cat = data[all_cat_cols].categorize()

from dask_ml.preprocessing import DummyEncoder
dum_enc = DummyEncoder()
dd_dum_enc = dum_enc.fit_transform(dd_cat)

dd = dd_dum_enc + train[final_num]

## Feature hasher did not work
#data_fh = train[featureHashList]
#from sklearn.feature_extraction import FeatureHasher
#dd_dum_enc = data_fh.categorize()
#dd_dum_enc = dum_enc.fit_transform(dd_dum_enc)
#for i in range(dd_dum_enc.shape[1]):
#    dd_dum_enc.iloc[:,i] = dd_dum_enc.iloc[:,i].astype('str')
#
#with parallel_backend('dask'):
#    # Your normal scikit-learn code here
#    fh = FeatureHasher(n_features=100, input_type='string')
#    dd_fh = fh.fit_transform(dd_dum_enc)
#dd_fh.toarray()

# Split into training and testing data
dftrain, dftest = dd.random_split([0.8, 0.2])

# Separate labels from data
train_labels = dftrain.HasDetections
test_labels = dftest.HasDetections

est = XGBClassifier()
est.fit(dftrain, train_labels)

prediction = est.predict(dftest)